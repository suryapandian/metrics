<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>eirik metrics</title>

	<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
	<meta name="author" content="Hakim El Hattab">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<link rel="stylesheet" href="http://localhost:43639/libs/reveal.js/4.1.3/reset.css">
	<link rel="stylesheet" href="http://localhost:43639/libs/reveal.js/4.1.3/reveal.css">

	
	
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

	  <!-- highlight Theme -->
  	
	  <link rel="stylesheet" href="http://localhost:43639/libs/highlight.js/11.3.1/styles/monokai-sublime.min.css">
	
	
		
	<link rel="stylesheet" href="http://localhost:43639/libs/reveal.js/4.1.3/plugin/chalkboard/style.css">
	
	
	
		<link rel="stylesheet" href="http://localhost:43639/libs/reveal.js/4.1.3/plugin/customcontrols/style.css">
	
	<link rel="stylesheet" href="http://localhost:43639/libs/styles/tasklist.css">



  <!-- Revealjs Theme -->
  
  	<link rel="stylesheet" href="http://localhost:43639/libs/reveal.js/4.1.3/theme/black.css" id="theme">
  
  


  <!-- Revealjs Theme -->
  

 
</head>

<body>
  


  <div class="reveal">

    <!-- Any section element inside of this container is displayed as a slide -->
    <div class="slides">

      


    
        <section >
            
            <h1>Metrics Training</h1>
<ul>
<li><a href="https://tl-eirik-albrigtsen.github.io/metrics">@tl-eirik-albrigtsen/metrics</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=evilz.vscode-reveal">vscode-reveal</a></li>
</ul>
<aside class="notes">
<ul>
<li>rapid fire about concepts - quickly and deeply into imporatnt topics, BUT</li>
<li>ultimately; this stuff requires a bit of trial and error hope is that by learning about these concepts that you at least know what is possible and can figure it out from there</li>
<li>additionally  links to various things available in slides - can follow along</li>
<li>obv: feel free to drop whenever - voluntary stuff - don’t stay here if it’s not useful</li>
</ul>
</aside>
            </section>
    



    
    <section>
        <section >
            <h3>What are metrics</h3>
<ul>
<li>collections of data points</li>
<li class="fragment">data points exposed by an app</li>
<li class="fragment">exposed on /metrics as text</li>
<li class="fragment">fetched by prometheus</li>
<li class="fragment">prometheus /15s :: GET /metrics</li>
</ul>
<aside class="notes">
<ul>
<li>metrics are a collection of data points</li>
<li>data points are snapshots of state exposed by the service</li>
<li>you expose it on /metrics, and something will continually pull that state</li>
<li>that something is called prometheus, it does the snapshotting.</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>What metrics looks like</h3>
<pre><code class="language-yaml"># TYPE http_requests_in_flight gauge
http_requests_in_flight 13
# TYPE http_request_seconds summary
http_request_seconds_sum{method=&quot;GET&quot;} 9036.32
http_request_seconds_count{method=&quot;GET&quot;} 807283.0
http_request_seconds_created{method=&quot;GET&quot;} 1605281325.0
http_request_seconds_sum{method=&quot;POST&quot;} 479.3
http_request_seconds_count{method=&quot;POST&quot;} 34.0
http_request_seconds_created{method=&quot;POST&quot;} 1605281325.0
# TYPE process_cpu_seconds counter
# UNIT process_cpu_seconds seconds
process_cpu_seconds_total 4.20072246e+06
</code></pre>
<aside class="notes">
<ul>
<li>plain text that your app exposes, two text formats, prometheus and openmetrics - but virtually identical (we use the old, and will move to the new one as we go along)</li>
<li>new format has UNIT instructions in the text</li>
<li>depending on language also possible to expose this via grpc, but diminishing returns to optimize, you get 4-8requests pre minute typically</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Metric terminology</h3>
<pre><code class="language-yaml"># TYPE http_requests_in_flight gauge
http_requests_in_flight 13
# TYPE http_request_seconds summary
http_request_seconds_sum{method=&quot;GET&quot;} 9036.32
http_request_seconds_count{method=&quot;GET&quot;} 807283.0
http_request_seconds_created{method=&quot;GET&quot;} 1605281325.0
http_request_seconds_sum{method=&quot;POST&quot;} 479.3
http_request_seconds_count{method=&quot;POST&quot;} 34.0
http_request_seconds_created{method=&quot;POST&quot;} 1605281325.0
# TYPE process_cpu_seconds counter
# UNIT process_cpu_seconds seconds
process_cpu_seconds_total 4.20072246e+06
</code></pre>
<aside class="notes">
<ul>
<li>the name on the left in green is the metric, the braces are label selectors, the white text is the value of the metric at that label point</li>
<li>if a metric has labels, then the metric has more than one dimension, in flight + process cpu has dimension 1, but the middle ones have two variant, a method dimension if you will</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
        <section >
            
            <h3>Metric Types</h3>
<ul>
<li class="fragment"><b>Gauge</b> - numerical value (can change)</li>
<li class="fragment"><b>Counter</b> - monotonically increasing number</li>
<li class="fragment"><b>Histogram</b> - counter with bucket dimensions</li>
</ul>
<aside class="notes">
<ul>
<li>we will do a little practical work here, show how all the metric types behave, and how to query them.</li>
<li>gauges are nice to plot by themselves no need for rates</li>
<li>counters easy to work with for app people, just call increment. monotonically increasing. prom tracks increases.</li>
<li>histogram - you can group, i.e. response times into buckets for classification</li>
</ul>
</aside>
            </section>
    



    
    <section>
        <section >
            <h3>Gauges</h3>
<p><img src="gauge_flight.png" alt=""></p>
<aside class="notes">
<ul>
<li>gauges are awkward, but can be useful</li>
<li>awkward because app often needs to compute global state to set them</li>
<li>and also coz prometheus polls every 15s, so you will not get the total amount of requests by looking at the changes here</li>
<li>the best you can answer with this metric here is: on average how many requests are in flight</li>
<li>gauges as you can see can be dimensional, here we have selected away one dimension, only focusing on the mutating ones, and we still get one per apiserver instance on kubernetes masters</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Gauges Sum</h3>
<p><img src="gauge_flight_sum.png" alt=""></p>
<aside class="notes">
<ul>
<li>if you have multiple gauges - that represent the same thing, you often just sum them together</li>
<li>did you know that even dev cluster, the api server has 4 mutating calls to kubernetes state?</li>
<li>so much for code freeze huh</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
    <section>
        <section >
            <h3>Counters</h3>
<p><img src="counter-viz.png" alt=""></p>
<p><small class="fragment">9 hour interval :: 0 -&gt; ~55000</small></p>
<aside class="notes">
<aside class="notes">
<ul>
<li>counters, by definition, monotonically increasing (hence increasing line)</li>
<li>app interface is to call counter.increment</li>
<li>resets happen when pods rotate, i.e. spot node goes down, or app crashes</li>
<li>here is a metric from the kubelet, how many cpu seconds prometheus is using</li>
<li>hard to read this in this form, but you can see from 19:00 to 04:00 prometheus went, on a fresh-pod, from using 0 cpu seconds (at boot), to ~ 55k</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Counters Rate</h3>
<p><img src="counter_rate.png" alt=""></p>
<p><small class="fragment">500 * 12 * 9 = 54000</small></p>
<aside class="notes">
<ul>
<li>because queries are always rooted in a time delta, we basically always are interested in the rate of change of these graphs</li>
<li>thus we wrap a rate or an increase around it to see the rate of change over a time interval</li>
<li>here 5m =&gt; on average, prometheus is using 500 cpu seconds every 5m</li>
<li>there are 12 x 5 minute intervals in an hour - math works out on red line cross ref</li>
<li>(this intuition only works with increase not rate - cover this here)</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Counters Sum Rate</h3>
<p><img src="counter_sumrate.png" alt=""></p>
<aside class="notes">
<ul>
<li>finally, because the lines are never overlapping (always only one -0 suffixed pod), we sum to reduce the dimensionality and get one generally continuous line</li>
<li>however, small point, see that the rate has now doubled when taking the sum?</li>
<li>care to guess why that is?</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Counters Correct Sum Rate</h3>
<p><img src="counter_sumrate_correct.png" alt=""></p>
<aside class="notes">
<ul>
<li>containes cpu usage seconds metric actually exposes subtly different values</li>
<li>one is the total for the pod, other is prometheus (main container)</li>
<li>go back show hard to miss lines, and sidecars at bottom</li>
<li>pick container you want in query</li>
<li>moral of the story is that your intuition w.r.t. SHOULD make sense when constructing queries</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
    <section>
        <section >
            <h3>Histograms</h3>
<ul>
<li>casual: “counter with a bucket dimension”</li>
<li>really: <code>$name_{sum,bucket,count}</code></li>
<li><a href="https://prometheus.io/docs/practices/histograms/">prometheus histograms</a></li>
</ul>
<p><img src="histogram_buckets.png" alt=""></p>
<aside class="notes">
<ul>
<li>a histogram is a multi-dimensional counter that has a bucket dimension</li>
<li>the bucket is typically named le (less than or equal - see pic)</li>
<li>name is technically the thing without the _bucket, because a histogram comes with a few auxillary metrics like <code>_sum</code> (sum of all the values) and <code>_count</code> a superfluous the count way to reference the less than infinity bucket</li>
<li>as a user, you just register the histogram in the app with buckets, and then just give it how long it took / how big what your measuring is, and the metric lib puts it in the bucket</li>
<li>TODO: own slide? what is this for? generally, quantile estimation. and depending on usage, it sometimes makes sense to compute quantiles in the app, but histograms is the easier, less-error prone alternative</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Histograms Quantiles</h3>
<p><img src="histogram_autocomplete.png" alt="">
<img src="histogram_quantile.png" alt=""></p>
<aside class="notes">
<ul>
<li>autocomplete in prometheus interface, type 0.95, tab, paste metric name</li>
<li>(grafana explore also has this type of autocomplete and they will even have little “did you mean histogram quantile?”)</li>
<li>visualising quantiles, here 0.95</li>
<li>time that 95% of requests responds to</li>
<li>BEST ESTIMATE, BUCKET DEPENDENT (threshold? somewhere between, avg is best guess)</li>
</ul>
</aside>
<!-- chapter 2 -->
            </section>
        

    </section>
    



    
        <section >
            
            <h3>What are metrics for</h3>
<ul>
<li class="fragment">is my app slower after upgrade?</li>
<li class="fragment">is it taking longer to respond to request than usual?</li>
<li class="fragment">what went wrong?</li>
</ul>
<aside class="notes">
<ul>
<li>metrics fundamentally here to answer questions about wtf is going on with your service</li>
<li>is it slower now than before?</li>
<li>it’s not necessarily enough to figure out WHY something is wrong, but points you at the right direction</li>
<li>and in the case of an incidents, breached acceptable thresholds on metrics is usually what starts an incident (then you can go look at logs, traces etc to figure out why and where)</li>
</ul>
</aside>
            </section>
    



    
        <section >
            
            <h3>Questions solvable by metrics</h3>
<ol>
<li class="fragment">state + trends of kubernetes objects</li>
<li class="fragment">resource usage of kubernetes objects</li>
<li class="fragment">capacity planning via on resource usage</li>
<li class="fragment">state + trends of your custom metrics</li>
<li class="fragment">SLOs + error budgets + burndown</li>
</ol>
<aside class="notes">
<ul>
<li>trends of some inherent property of the objects (such as pods alive, deployments receiving traffic)</li>
<li>but also trends on resource usage</li>
<li>and you can use these resource usage metrics proactively to project when you are going to run out.</li>
<li>but importantly though; you can expose your own business metrics</li>
<li>and these are typically what is used in more complex things like request availability, failure rates, which gets further defined in SLOs (we’ll hit on this again later)</li>
</ul>
</aside>
            </section>
    



    
    <section>
        <section >
            <h3>Kubernetes object metrics</h3>
<ul>
<li><a href="https://prometheus-underlying.truelayer.com/graph?g0.expr=sum(kube_cronjob_status_active%7Bcronjob%3D%22security-checks-host-vulnerabilities%22%7D)&amp;g0.tab=0&amp;g0.stacked=0&amp;g0.show_exemplars=0&amp;g0.range_input=6h">is my <code>Cronjob</code> running</a>?</li>
<li><a href="https://grafana.t7r.dev/d/alJY6yWZz/kubernetes-horizontal-pod-autoscaler">how does my <code>HPA</code> actually scale</a>?</li>
<li><a href="https://prometheus-underlying.truelayer.com/graph?g0.expr=increase(kube_pod_container_status_restarts_total%7Bjob%3D%22kube-state-metrics%22%2C%20namespace%3D~%22.*%22%7D%5B10m%5D)%20%3E%200%0Aand%0Akube_pod_container_status_waiting%7Bjob%3D%22kube-state-metrics%22%2C%20namespace%3D~%22.*%22%7D%20%3D%3D%201&amp;g0.tab=0&amp;g0.stacked=0&amp;g0.show_exemplars=0&amp;g0.range_input=6h">is my <code>Pod</code> been crashlooping</a>?</li>
</ul>
<p class="fragment">beats: kubectl get X -oyaml</p>
<aside class="notes">
<ul>
<li>snapshots of kubernetes state - via kube-state-metrics</li>
<li>DEPLOY: enough replicas? mismatches beween desired/actual? (spec/status)</li>
<li>HPA: how does hpa driven replica count cycle over time</li>
<li>POD: if you paid attention last week during dns issues - you likely saw these</li>
<li>All in theory reproducible locally, if you hit up enter every 15s</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Resource usage of k8s objects</h3>
<ul>
<li><a href="https://grafana.t7r.dev/d/6581e46e4e5c7ba40a07646395ef7b23/kubernetes-compute-resources-pod">how much memory/cpu/disk used by pods</a></li>
<li><a href="https://grafana.t7r.dev/d/7a18067ce943a40ae25454675c19ff5c/kubernetes-networking-pod">how much IO used by pods</a></li>
<li>compare: resource vs. requests/limits</li>
</ul>
<aside class="notes">
<ul>
<li>via kubelet state (how much memory cpu your pods use)</li>
<li>IO: (iops, watches, bandwidth/packets rx/tx/dropped - node-exporter)</li>
<li>compare: kube-state vs. kubelet metrics</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Resource usage of aggregated k8s</h3>
<ul>
<li><a href="https://grafana.t7r.dev/d/a164a7f0339f99e89cea5cb47e9be617/kubernetes-compute-resources-workload">mem/cpu/disk</a>/<a href="https://grafana.t7r.dev/d/728bf77cc1166d2f3133bf25846876cc/kubernetes-networking-workload">IO</a> usage for a deployment</li>
<li><a href="https://grafana.t7r.dev/d/200ac8fdbfbb74b39aff88118e4d1c2c/kubernetes-compute-resources-node-pods">…utilization on a node</a></li>
<li><a href="https://grafana.t7r.dev/?orgId=1&amp;search=open&amp;query=cluster">…utilization on a cluster</a></li>
</ul>
<aside class="notes">
<ul>
<li>can use these same metrics and aggregate across various kubernetes types</li>
<li>workloads(deployments - the ones you care about)</li>
<li>nodes (saturation of owning node)</li>
<li>clusters (need more nodes? devops metric)</li>
</ul>
</aside>
            </section>
        
            <section >
                <h4>Kubernetes object metrics?</h4>
<p><i>What do you investigate with kubectl?</i></p>
<p>→ <a href="https://prometheus-underlying.truelayer.com/graph">prometheus dev</a> → kube_</p>
<aside class="notes">
<ul>
<li>GOTO dev prometheus, type <code>kube_</code> select kube object type, see what’s available</li>
<li>take 5 min, answer questions as best as possible</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Trends on your custom metrics</h3>
<p>TODO: better stuff here… link up with SLO stuff</p>
<pre><code class="language-yaml"># METRICS
http_server_sli_total{sli_error_type=~&quot;.+&quot;}
pagerduty_routing_outcome_total{outcome=&quot;...&quot;}
</code></pre>
<pre><code class="language-css">sum(increase(alert_routing_outcome_total{}[$interval]))
by (outcome)
</code></pre>
<pre><code class="language-css">sum(rate(alert_routing_outcome_total{outcome=&quot;success&quot;}[1h]))
  /
sum(rate(alert_routing_outcome_total[1h]))
</code></pre>
<aside class="notes">
<ul>
<li>The most important category; 2 custom metrics that tracks standard SLI + a custom SLI (bottom)</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
    <section>
        <section >
            <h3>SLOs</h3>
<ul>
<li class="fragment"><b>SLO</b>: Service Level Objective (the want)</li>
<li class="fragment"><b>SLI</b>: Service Level Indictor (the metric)</li>
<li class="fragment"><b>SLA</b>: Service Level Agreement (the contract)</li>
<li class="fragment"><b>Error Budget</b>: Allowed failure time</li>
</ul>
<aside class="notes">
<ul>
<li>SLO: …objective that we want to be successful (pod uptime, request availability, success rate)</li>
<li>SLI: service level indicator - a metric that we can track over time to determine if app satisfing an objective</li>
<li>SLA: a promise of maintaining an SLO for at least N% of the time monthly/yearly</li>
<li>EB: complement of SLA over time period (e.g. 99.95% avail -&gt; 4.38 hours per year, 43s/day)</li>
<li>SRE book wisdom: track EB and slow down when you are using too much of it</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>SLOs TrueLayer</h3>
<ul>
<li class="fragment"><a href="https://paper.dropbox.com/doc/Availability-as-a-SLI--BDgeUkSCfcjodjnpn~LEMxg7Ag-AtRCn3Z6C7cRQ1v5bNmIa">Availability as SLI</a></li>
<li class="fragment"><a href="https://github.com/TrueLayer/truelayer-architecture/blob/main/adrs/0020-rest-api-metrics.md">ADR-20</a></li>
<li class="fragment"><a href="https://github.com/TrueLayer/truelayer-architecture/blob/main/adrs/0025-grpc-api-metrics.md">ADR-25</a></li>
<li class="fragment"><a href="https://grafana.truelayer.com/d/ASFR126Wd/rest-api-shared-metrics?orgId=1">Shared REST Metrics Dashboard</a></li>
<li class="fragment"><a href="https://github.com/slok/sloth">sloth</a></li>
</ul>
<p><small class="fragment"><a href="https://app.getguru.com/card/TdEodqxc/Tracking-SLOs-via-Honeycomb?q=slo">…via HoneyComb</a></small></p>
<aside class="notes">
<ul>
<li>classified error types in 2020 (paper doc)</li>
<li>made ADRs in 2021 - re-use of metric setups to allow dashboard reuse</li>
<li>plumbing done, but still some edge cases</li>
<li>TODO: verify clamp / absent</li>
<li>TODO: verify space usage? why are they slow?</li>
<li>TODO: show some big queries…
works for now, but imperfect. better to use honeycomb in the future</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
    <section>
        <section >
            <h3>Advanced Query Functionality</h3>
<ul>
<li><a href="https://prometheus.io/docs/prometheus/latest/querying/functions/">querying functions</a></li>
<li><a href="https://prometheus.io/docs/prometheus/latest/querying/operators/">querying operators</a></li>
</ul>
<aside class="notes">
<ul>
<li>best docs here are in these two docs has all the functionality</li>
<li>will quickly blast through some things here</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Query Functions</h3>
<ul>
<li>rate/increase</li>
<li>sum/max/min/topk :: by/without</li>
<li>{aggregation}_over_time (avg, min, max, sum) <!-- count stddev> --></li>
<li>clamp_min + clamp_max = clamp</li>
<li>predict_linear / deriv / delta</li>
<li>math: sqrt, sgn, scalar, round, floor/ceil, logs, exp</li>
<li>comparison: &gt; &lt; &lt;= &gt;= == !=</li>
<li>absent</li>
</ul>
<aside class="notes">
<ul>
<li>rate/increase - same except increase multiplies by time unit to get accurate wyw if value focus</li>
<li>sums can be done with a (by) or (without) clause, to drop dimensionality</li>
<li>sum drops ALL dimensions, sum by keeps 1, sum without drops only specifics</li>
<li>avg_over_time, min_over_time - sometimes easier to read and make more sense MATH</li>
<li>clamps are MIN, MAX fns</li>
<li>TODO: advanced stuff, predict + absent</li>
<li>comparisons: converts to bool stream: 0 or 1</li>
<li>comparisons need to handle edge cases where stuff is absent</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Query Operators 1 Logical</h3>
<ul>
<li><a href="https://prometheus-underlying.t7r.dev/graph?g0.expr=node_timex_offset_seconds%20%3E%200.0005%0A%20and%0Aderiv(node_timex_offset_seconds%5B5m%5D)%20%3E%200&amp;g0.tab=1&amp;g0.stacked=0&amp;g0.show_exemplars=0&amp;g0.range_input=1h&amp;g1.expr=node_timex_offset_seconds%20%3E%200.0005&amp;g1.tab=1&amp;g1.stacked=0&amp;g1.show_exemplars=0&amp;g1.range_input=1h&amp;g2.expr=deriv(node_timex_offset_seconds%5B5m%5D)%20%3E%200&amp;g2.tab=1&amp;g2.stacked=0&amp;g2.show_exemplars=0&amp;g2.range_input=1h">and (intersection)</a> / or (union) / unless (complement)</li>
</ul>
<aside class="notes">
<ul>
<li>did one AND before. container restarts, and between kube_pod_container_status_waiting and kube_pod_container_status_restarts changing</li>
<li>way these works is that they match exact labels</li>
<li>(set operation on label matches)</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Query Operators 2 Joins</h3>
<ul>
<li><a href="https://prometheus-underlying.t7r.dev/graph?g0.expr=(kube_job_failed%7Bjob%3D%22kube-state-metrics%22%2C%20namespace%3D~%22.%2B%22%7D%20%3D%3D%201)%20*%0Aon(job_name)%20group_left(label_app_kubernetes_io_name)%0Akube_job_labels%7Bjob%3D%22kube-state-metrics%22%2C%20label_app_kubernetes_io_name%3D~%22.%2B%22%7D%0A&amp;g0.tab=1&amp;g0.stacked=0&amp;g0.show_exemplars=0&amp;g0.range_input=1h&amp;g1.expr=(kube_job_failed%7Bjob%3D%22kube-state-metrics%22%2C%20namespace%3D~%22.%2B%22%7D%20%3D%3D%201)&amp;g1.tab=1&amp;g1.stacked=0&amp;g1.show_exemplars=0&amp;g1.range_input=1h&amp;g2.expr=kube_job_labels%7Bjob%3D%22kube-state-metrics%22%2C%20label_app_kubernetes_io_name%3D~%22.%2B%22%7D&amp;g2.tab=1&amp;g2.stacked=0&amp;g2.show_exemplars=0&amp;g2.range_input=1h">on</a> / ignoring</li>
</ul>
<pre><code class="language-promql">&lt;vector expr&gt; &lt;bin-op&gt; ignoring(&lt;label list&gt;) &lt;vector expr&gt;
&lt;vector expr&gt; &lt;bin-op&gt; on(&lt;label list&gt;) &lt;vector expr&gt;
</code></pre>
<aside class="notes">
<ul>
<li>these are hard AF. generally you are doing joins to get extra labels</li>
<li>ex: LHS has root labels, group left (LEFT OUTER JOIN) on job_name</li>
<li>and add label_app from that</li>
<li>refine namespace to get less matches</li>
</ul>
</aside>
<!-- chapter 3 -->
            </section>
        

    </section>
    



    
        <section >
            
            <h3>Optimizing Queries</h3>
<p><img src="timeouts.png" alt=""></p>
<ul>
<li class="fragment">cardinality</li>
<li class="fragment">resolution</li>
<li class="fragment">recording</li>
</ul>
<aside class="notes">
<ul>
<li>so you are here</li>
<li>why would prometheus do this</li>
<li>we will cover 3 optimization strategies</li>
<li>cardinalit: easiest to solve at design phase</li>
<li>resolution: using less greedy queries</li>
<li>recording rules: a hammer to help the hard cases</li>
</ul>
</aside>
            </section>
    



    
    <section>
        <section >
            <h3>Cardinality</h3>
<hr>
<pre><code class="language-yaml">http_request_sum{path=&quot;/api/v1&quot;,method=&quot;GET&quot;} 9036.32
http_request_sum{path=&quot;/api/v2&quot;,method=&quot;GET&quot;} 3036.1
http_request_sum{path=&quot;/api/v2&quot;,method=&quot;DELETE&quot;} 1.3
http_request_sum{path=&quot;/api/v1&quot;,method=&quot;POST&quot;} 4479.3
http_request_sum{path=&quot;/api/v2&quot;,method=&quot;POST&quot;} 479.3
</code></pre>
<pre><code class="language-py">cardinality(http_request_sum)
  = len({&quot;GET&quot;, &quot;POST&quot;, &quot;DELETE&quot;}) *
    len({&quot;/api/v1&quot;, &quot;/api/v2&quot;})
  = 2*3 = 6
count(http_request_sum) = 5
</code></pre>
<aside class="notes">
<ul>
<li>to determine the cardinality of a metric, multiple the size of dimensions together</li>
<li>cardinality is a math way of measuring the cartesian products of set sizes</li>
<li>we generally shorthand cardinality as how large the metric can grow to be if you include all possible value combinations (count tends to converge towards the cardinality)</li>
<li>so can in theory determined this up front - entirely dependent on your dimensions/labels</li>
<li>but in practice, we have some unknowns: some labels get injected later “enriched” like pod labels</li>
<li>also what if people type arbitrary urls in your web app, do you get a new metric for that?</li>
<li>anyway, this is very mathsy, but we’ll go a little further</li>
</ul>
</aside>
            </section>
        
            <section >
                <!--
### Cardinality of Histograms

`http_request_duration_seconds_bucket`

<ul>
<li class="fragment"><b>le buckets</b> :: 41</li>
<li class="fragment"><b>service names</b> :: 33</li>
<li class="fragment"><b>app names</b> :: 28</li>
<li class="fragment"><b>status code</b> :: 20</li>
<li class="fragment"><b>action</b> :: 98</li>
<li class="fragment"><b>controller</b> :: 66</li>
</ul>

<strikeout class="fragment">uh... 41x33x28x20x98x66? ~= 5 billion..?</stikeout>
<p class="fragment">count(http_request_duration_seconds_bucket) = 100,000</p>


<aside class="notes">

- really hard to estimate, buckets are not standardised across apps
- so cardinalities are different for app subsets of each of them
- count by app, 2 apps account for 80k of that..
- super-metrics a bit of a double edged sword, useful for standardised panels, but makes it hard to see where the cardinality is coming from

</aside>

-->
<h4>Cardinality of Histograms</h4>
<p><code>http_request_duration_seconds_bucket</code></p>
<ul>
<li class="fragment"><b>100 pods with 10 buckets</b> :: 1000 series</li>
<li class="fragment"><b>10 endpoints</b> :: 10,000 series</li>
<li class="fragment"><b>10 status codes</b> :: 100,000 series</li>
<li class="fragment"><b>4 http methods</b> :: 400,000 series</li>
<li class="fragment"><b>100 tenants</b> :: 40,000,000 series</li>
</ul>
<aside class="notes">
<ul>
<li>this is a worst type case calculation, we have &lt;10M time series in prod</li>
<li>AFTER GOING THERE, COME BACK, WHY DOES THIS GIVE YOU THIS MANY?</li>
<li>well, can calculate the histogram per pod per endpoint, do you need that?</li>
<li>can calculate histogram per pod per method, do you need that?</li>
<li>are you just interested in the avg latency per tenant?</li>
<li>…you can do this in 100*100 = 10k series</li>
<li>how? drop buckets, drop endpoints, drop status codes, drop methods</li>
<li>and do</li>
<li>100 tenants with just 100 pods and 10 buckets is 100k</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Why care about cardinality</h3>
<ul>
<li>lower cardinality => faster response</li>
<li class="fragment">high cardinality => slow response / timeout</li>
<li class="fragment"><b>try to stay below 10k</b> per metric</li>
</ul>
<p><a class="fragment" href="https://promcon.io/2019-munich/slides/containing-your-cardinality.pdf">contain your cardinality</a></p>
<aside class="notes">
<ul>
<li>between 10k and 100k, it starts becoming impractical to do arbitrary queries on the data</li>
<li>above 100k you have to restrict yourself to tiny subsets, if you are even able to get a response at all</li>
<li>think about it, if you are asking prometheus to reduce &gt;10k data points to show a single line in a panel, you’ve missed a chance to optimize</li>
<li>most of the time our prometheus is busy doing huge queries</li>
<li>some of it comes with our scale, but some of it is definitely unavoidable</li>
</ul>
</aside>
            </section>
        
            <section >
                <h4>Cardinality Optimization</h4>
<ul>
<li><b>100 pods with 10 buckets</b> :: 1000 series</li>
<li><b>10 endpoints</b> :: 10,000 series</li>
<li><b>10 status codes</b> :: 100,000 series</li>
<li><b>4 http methods</b> :: 400,000 series</li>
<li><b>100 tenants</b> :: 40,000,000 series</li>
</ul>
<br>
<ul class="fragment">
<li>http_request_total : pods,ep,code = 10k</li>
<li>http_request_bucket : pods,buckets = 1k</li>
<li>http_request_ep_bucket : buckets*ep*method = 1k</li>
<li>http_tenant_requests_total : code,tenants = 1k</li>
</ul>
<aside class="notes">
<ul>
<li>AFTER GOING THERE, COME BACK, WHY DOES THIS GIVE YOU THIS MANY?</li>
<li>well, can calculate the histogram per pod per endpoint, do you need that?</li>
<li>can calculate histogram per pod per method, do you need that?</li>
<li>are you just interested in the avg latency per tenant?</li>
<li>…you can do this in 100*100 = 10k series</li>
<li>how? split the metrics</li>
<li>the first two are easy and gives you 90%</li>
<li>the last two are hard because it’s non-trivial to remove the pod labels</li>
<li>generally we advise not including tenants and theoretically unbounded dimensions in metrics</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
    <section>
        <section >
            <h3>Resolution</h3>
<ul>
<li>12d at 30s resolution is 1440 points</li>
<li>3d at 1m resolution is 4320 points</li>
<li>7d at 5m resolution is 2000 points</li>
<li>1 panel &lt; 1000 pixels</li>
</ul>
<p class="fragment">resolution rule of thumb:</p>
<ul class="fragment">
<li>1m for < 3h</li>
<li>5m for < 12h</li>
<li>1h for > 12h</li>
</ul>
<aside class="notes">
<ul>
<li>we all ask for too much data, but we can never use all of that data, not enough pixels</li>
<li>a panel has 1000 pixels in general, maybe 2-3k on a 4k screen if you really blow one up, and you generally would not notice any behavioural difference if it had just 200-500 DP (trends &gt; pixel accurate line)</li>
<li>crucially tho: low res is much faster to retrieve, and high res is slow</li>
<li>so set those intervals according to when you are browsing - here’s a rule of thumb</li>
<li>these resolutions not arbitrary; they correspond to how we store series in thanos long term (outside a couple of weeks, we only have 1h)</li>
<li>but… how do we make the interval dynamic? it’s in the query…</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Grafana Resolution Interval</h3>
<p><img src="grafana_interval.png" alt=""></p>
<aside class="notes">
<ul>
<li>have intervals parmaetrised in your dashboards - e.g. promload</li>
<li>pass it as $interval in rates</li>
<li>super helpful for intuition, use it all the time, also: increase vs rate</li>
<li>also important to understand what happens when you change the interval?</li>
<li>NB: graphs will often change quite drastically in response to it (higher time unit - smoother, looses fine detail - but nicer on the eyes)</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Grafana Resolution</h3>
<p><img src="grafana_resolution.png" alt=""></p>
<aside class="notes">
<ul>
<li>also whack the resolution to a half or a quarter, you’re not going to notice a difference</li>
<li>really hard to meaningfully distinguish between high res and low res - and the speed improvement is more important</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
        <section >
            
            <h3>Measure</h3>
<p><img src="promethes_timing.png" alt=""></p>
<aside class="notes">
<ul>
<li>prometheus will tell you if it’s slow</li>
<li>optimize, limit large fetches if you don’t need it for your visualization</li>
<li>note that mvp is usually fast - slower once there’s more data, &lt;.1s is good, &lt;1s is OK</li>
</ul>
</aside>
            </section>
    



    
    <section>
        <section >
            <h3>It’s still slow? Recording rules</h3>
<pre><code class="language-yaml">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus-stack-kube-prom-k8s.rules
spec:
  groups:
  - name: k8s.rules
    rules:
</code></pre>
<pre><code class="language-yaml">- expr: |-
    sum by (cluster, namespace, pod, container) (
      irate(container_cpu_usage_seconds_total{
        job=&quot;kubelet&quot;,
        metrics_path=&quot;/metrics/cadvisor&quot;,
        image!=&quot;&quot;}[5m]
      )
    ) * on (cluster, namespace, pod)
    group_left(node) topk by (cluster, namespace, pod) (
      1, max by(cluster, namespace, pod, node)
        (kube_pod_info{node!=&quot;&quot;})
    )
  record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
</code></pre>
<p><a href="https://prometheus-underlying.t7r.dev/graph?g0.expr=node_namespace_pod_container%3Acontainer_cpu_usage_seconds_total%3Asum_irate%7Bnamespace%3D%22monitoring%22%2C%20pod%3D~%22.*prom-prometheus.*%22%7D&amp;g0.tab=0&amp;g0.stacked=0&amp;g0.show_exemplars=0&amp;g0.range_input=1h">usage</a></p>
<aside class="notes">
<ul>
<li>hammer that can fix things to factor out heavy queries tha twe need to do often</li>
<li>need a costly query in an alert or a very common dashboard?</li>
<li>write a record, and have the alert + dashboard re-use the record</li>
<li>deindented here for clarity, enormous expression to calculate the cpu seconds</li>
<li>deals with the pitfall i mentioned earlier, and you see it puts a record field</li>
<li>convention is to use a consistent naming convention with categories separated by :</li>
<li>here, node_level_resource colon metric_on_resourc colon how_it_is_measued</li>
<li>so at least try to stick to a reasonable convention for it (examples in mixins)</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Recording Rules Info</h3>
<ul>
<li>provides a way to do costly steps</li>
<li>evaluation cost amortised</li>
<li>uses “sum() by” to reduce dimensions</li>
</ul>
<aside class="notes">
<ul>
<li>prometheus is continually evaluating the expression so that you can get a fast result</li>
<li>it is using dimensional reduction we talked about earlier</li>
<li>leaving a dimensional series with substantial size still - but nothing needs to be done to it</li>
<li>effectively we just fetch the row we want</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Recording Rule Optimization</h3>
<ul>
<li>amortization =&gt; cost affects everyone</li>
<li>rolling computation must be reasonably quick</li>
<li>15s (default),30s,1m+ configurable <a href="https://github.com/TrueLayer/gitops-monitoring/blob/f222a6d9d527e0b82c46cb78fd08b9527def7f75/development/prometheus-usage-rules.yaml#L13">interval</a></li>
</ul>
<p>see <a href="https://prometheus-underlying.t7r.dev/rules">prometheus/rules</a>
<img src="rule_evaltime_k8s.png" alt=""></p>
<aside class="notes">
<ul>
<li>constant passive load on the system: not free - but it’s the same as alerts</li>
<li>if you have a complex query in an alert or an important dashboard, MAKE A RULE</li>
<li>if resolution allows, you can set interval to 30s, or 1m - 15s default</li>
<li>NB: 10s eval in 15s interval =&gt; occupying 2/3rds of a core - so please try not to do that</li>
<li>most rule groups take milliseconds to complete</li>
<li>IMG: exception: kubernetes rules - main cluster rules</li>
<li>HOWEVER: ∃ ~20 ob-uk related alerts that take longer than that (because they are not using recording rules and has complicated alerts - so cheaper to use rules - and you get fast dashboards)</li>
<li>uki, ping us</li>
</ul>
</aside>
<!-- chapter 4 -->
            </section>
        

    </section>
    



    
    <section>
        <section >
            <h3>How to not reinvent the wheel</h3>
<p>you <strong>do not need to</strong>:</p>
<ul>
<li class="fragment">make panels for cpu/mem viz</li>
<li class="fragment">make alerts for generic pod failures</li>
<li class="fragment">figure out how to visalize histograms</li>
</ul>
<aside class="notes">
<ul>
<li>you can link to the mixin dashboards for Kubernetes Compute (id contant in gitops)</li>
<li>you can rely on generic mixin alerts KubePod</li>
<li>you can copy paste panels from across dashboards and change the metric names and units</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Linking to cpu/mem viz</h3>
<p><a href="https://grafana.t7r.dev/d/a164a7f0339f99e89cea5cb47e9be617/kubernetes-compute-resources-workload?orgId=1&amp;var-datasource=Prometheus&amp;var-cluster=&amp;var-namespace=monitoring&amp;var-type=statefulset&amp;var-workload=prometheus-prometheus-stack-kube-prom-prometheus&amp;from=now-3h&amp;to=now">compute workload board</a></p>
<p><a href="https://grafana.t7r.dev/d/opspromload/prometheus-load?orgId=1">demo with links</a></p>
<aside class="notes">
<ul>
<li>go through dashboard: super parametrised - drills down to pods</li>
<li>show how to link</li>
<li>contrast the 3 different types of links - table one superior</li>
<li>all can MAINTAIN CURRENT TIME</li>
<li>all can FORWARD VARIABLES</li>
<li>copy the Button link into nginx ingress</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Generic alerts</h3>
<p>maybe just go through them? and give them a link to the big fuckoff one?
IMAGE FROM ALERTIPLEX</p>
<aside class="notes">
<ul>
<li>KubePodNotReady, KubeContainersWaiting</li>
<li>KubeDeploymentReplicasMismatch</li>
</ul>
</aside>

            </section>
        
            <section >
                <h3>Stealing Panels</h3>
<p><a href="https://grafana.com/blog/2020/06/23/how-to-visualize-prometheus-histograms-in-grafana/">learn</a> vs.
<a href="https://grafana.truelayer.com/d/ASFR126Wd/rest-api-shared-metrics?orgId=1&amp;var-deployment=auth-server&amp;var-interval=5m&amp;from=now-3h&amp;to=now">copy</a></p>
<aside class="notes">
<ul>
<li>here is a huge blogpost on grafana on how to use heatmaps around prometheus histograms</li>
<li>enormous, don’t want to do any of this - i just want my data</li>
<li>i want to copy this panel, navigate to devx/self-serve-backend</li>
<li>fix deployment - and missing interval - align with latency</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
    <section>
        <section >
            <h2>Dashboard Timeseries Panel</h2>
<p>Things you can and should use or configure:</p>
<p><small class="fragment">or steal</small></p>
<ul>
<li class="fragment">cross-link</li>
<li class="fragment">axes on panel</li>
<li class="fragment">units</li>
<li class="fragment">legends</li>
<li class="fragment">display prop</li>
</ul>
<aside class="notes">
<ul>
<li>since we’ve started faffing around on grafana…</li>
<li>things you can and definitely should configure (OR STEAL)</li>
<li>showed links; display (pretty), axes (what does it do), units (how many what?), legends (summaries)</li>
<li>legend note: people tend to want summaries by introducing a million little stat panels - which is not ideal, they’re harder to read, and not interactive like time series</li>
<li>so we will focus on TIME SERIES (MAIN GRAFANA PANEL - replaces legacy Graph - safe to upgrade)</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Axes + Units + Legends</h3>
<p>TODO: find demo dashboard</p>
<aside class="notes">
<ul>
<li>time series first of all - old graph is legacy - this is bread and butter 90% use this</li>
<li>one panel should have consistent axes and units - fundamental properties</li>
<li>MAX/MIN, FUCKING UNIT</li>
<li>legend use</li>
<li>display vis-a-vis a sum by - to illustrate legend reductions</li>
<li>show stack with hue to show how they partition the space</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Display Properties</h3>
<aside class="notes">
<ul>
<li>display overrides (color for properties, names)</li>
<li>it’s a stupid amount of customization here, and you can kind of spend way too much time on pointless things in here (running dashboard)</li>
<li>overrides by regex (shared dashboard)</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
    <section>
        <section >
            <h3>Other Panels</h3>
<ul>
<li>Heatmap</li>
<li>Table</li>
<li>Plot</li>
</ul>
<aside class="notes">
</aside>
            </section>
        
            <section >
                <h3>Tables</h3>
<ul>
<li>Dynamic linking to resource use</li>
<li>Show reduced info</li>
</ul>
<aside class="notes">
<ul>
<li>TODO: link to promload for first table</li>
<li>TODO: link to compute dashboard for last table</li>
<li>TODO: image?</li>
</ul>
</aside>
            </section>
        
            <section >
                <h3>Heatmap</h3>
<pre><code class="language-css">histogram_quantile(0.99, sum(rate(
    http_server_request_duration_seconds_bucket
{app=~&quot;[[deployment]]&quot;}[$interval])) by (le))
</code></pre>
<p><img src="histograms.png" alt=""></p>
<pre><code class="language-css">sum(increase(http_server_request_duration_seconds_bucket
 {app=~&quot;[[deployment]]&quot;}[$interval])) by (le)
</code></pre>
<aside class="notes">
Histograms over time
<ul>
<li>two ways of visualizing the same time - prod latencies of ONE service</li>
<li>first is doing a histogram_quantile TODO: link to prometheus where we show this?</li>
<li>second is doing a histogram heatmap that we showed (count represents the interval rate)</li>
<li>query is similar, but it’s formatted as a heatmap (many pitfalls, so steal this)</li>
</ul>
</aside>
            </section>
        

    </section>
    



    
        <section >
            
            <h3>Alignment</h3>
<p><img src="alignment.png" alt="">
(<a href="https://grafana.t7r.dev/d/alJY6yWZz/kubernetes-horizontal-pod-autoscaler?orgId=1&amp;from=1643792014817&amp;to=1643801739120">hpa dashboard</a>)</p>
<aside class="notes">
<ul>
<li>align panels that show similar things - quickly highlights when they are out of sync</li>
<li>HPA dashboard; deployment that is scaling on metrics, 15min spike of CPU usage, long tail of containers</li>
<li>are we too slow to scale down? depends on how long it takes to boot. and how big spikes we get.</li>
</ul>
</aside>
            </section>
    



    
        <section >
            
            <h2>tips and tricks: formatting/thresholds/severity/silences</h2>
<aside class="notes">
</aside>
            </section>
    



    
        <section >
            
            <h3>gitops</h3>
<ul>
<li>edit live - copy json</li>
<li>create live - copy json</li>
</ul>
<p>then <code>./dashboard.sh my-folder</code></p>
<aside class="notes">
<p>just edit on dev grafana. stash exists for collab, but you don’t need to think about it.
can’t save on main grafanas
show urls.</p>
<p>you can work on dev regardless though!
afraid to lose your work? make a branch. continually commit to it with <code>./dashboard.sh folder</code> or <code>./dashboard-docker.sh folder</code></p>
</aside>
            </section>
    



    
        <section >
            
            <h3>Scraping / ServiceMonitors</h3>
<p><a href="https://app.getguru.com/card/TyRzye5c/How-to-get-your-metrics-into-Prometheus?q=alertmanager">Guru: How to get your metrics into Prometheus</a></p>
<p>Don’t use metricsScraping labels</p>
<aside class="notes">
<ul>
<li>elided talking about this</li>
<li>only affects a few operators - we have standardised scrapers</li>
</ul>
</aside>
            </section>
    


    </div>


  </div>

  	
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/reveal.js"></script>
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/zoom/zoom.js"></script>
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/notes/notes.js"></script>
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/search/search.js"></script>
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/markdown/markdown.js"></script>
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/highlight/highlight.js"></script>
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/menu/menu.js"></script>
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/math/math.js"></script>

	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/fullscreen/plugin.js"></script>
  
  	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/animate/plugin.js"></script>
  	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/animate/svg.min.js"></script>
  
  	 <!--	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/anything/plugin.js"></script> -->

 <!--	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/audio-slideshow/plugin.js"></script>  -->
<!--	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/audio-slideshow/recorder.js"></script>-->
<!--	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/audio-slideshow/RecordRTC.js"></script>-->


	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/customcontrols/plugin.js"></script>
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/embed-tweet/plugin.js"></script>

	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/chart/chart.min.js"></script>
	<script src="http://localhost:43639/libs/reveal.js/4.1.3/plugin/chart/plugin.js"></script>

  <script>

		const printPlugins = [
			RevealNotes, 
			RevealHighlight,
			RevealMath,
			RevealAnimate,
			
			RevealEmbedTweet,
			RevealChart,
		];

		const plugins =  [...printPlugins,
		RevealZoom, 
		RevealSearch, 
				RevealMarkdown, 
				RevealMenu, 
				RevealFullscreen,
				//RevealAnything,
				//RevealAudioSlideshow,
				//RevealAudioRecorder,
				RevealCustomControls, 
				// poll
				// question
				// seminar
				 ]


		// Also available as an ES module, see:
		// https://revealjs.com/initialization/
		Reveal.initialize({
			controls: true,
			controlsTutorial: true,
			controlsLayout: 'bottom-right',
			controlsBackArrows: 'faded',
			progress: true,
			slideNumber: false,
			//#showSlideNumber "all" "print" "speaker"
			hash: true,//#  hash: false,
			//# respondToHashChanges: true,
			//# history: false,
			keyboard: true,
			//#keyboardCondition: null,
			overview: true,
			center: true,
			touch: true,
			loop: false,
			rtl: false,
			//#navigationMode: 'default', linear grid
			shuffle: false,
			fragments: true,
			fragmentInURL: false,
			embedded: false,
			help: true,
			//#pause: true
			showNotes: false,
			autoPlayMedia: false, // TODO fix this to a nullable value
			//#preloadIframes: null. true false
			//#autoAnimate: true
			//#autoAnimateMatcher: null,
			//#autoAnimateEasing: 'ease',
			//autoAnimateDuration: 1.0,
			//#autoAnimateUnmatched: true
			//#autoAnimateStyles: []
			autoSlide: 0, // TODO fix this to a falseable value
			autoSlideStoppable: true,
			autoSlideMethod: '0',
			defaultTiming: 120,
			mouseWheel: false,
			//#previewLinks: false
			//#postMessage: true,  // TODO : this can cause issues with the vscode api ???
			//#postMessageEvents: false,
			//#focusBodyOnPageVisibilityChange: true,
			transition: 'slide',
			transitionSpeed: 'fast',
			backgroundTransition: 'none',
			//#pdfMaxPagesPerSlide: Number.POSITIVE_INFINITY,
			//#pdfSeparateFragments: true,
			//#pdfPageHeightOffset: -1,
			viewDistance: 3,
			//#mobileViewDistance: 2,
			display: 'block',
			//#hideInactiveCursor: true,
			//#hideCursorTime: 5000

			// Parallax Background
			parallaxBackgroundImage: '',
			parallaxBackgroundSize: '',
			parallaxBackgroundHorizontal: 0,
			parallaxBackgroundVertical: 0,
			
			//Presentation Size
			width: 960,
			height: 700,
			margin: 0.04,
			minScale: 0.2,
			maxScale: 2,
			disableLayout: false,

			audio: {
				prefix: 'audio/', 	// audio files are stored in the "audio" folder
				suffix: '.ogg',		// audio files have the ".ogg" ending
				textToSpeechURL: null,  // the URL to the text to speech converter
				defaultNotes: false, 	// use slide notes as default for the text to speech converter
				defaultText: false, 	// use slide text as default for the text to speech converter
				advance: 0, 		// advance to next slide after given time in milliseconds after audio has played, use negative value to not advance
				autoplay: false,	// automatically start slideshow
				defaultDuration: 5,	// default duration in seconds if no audio is available
				defaultAudios: true,	// try to play audios with names such as audio/1.2.ogg
				playerOpacity: 0.05,	// opacity value of audio player if unfocused
				playerStyle: 'position: fixed; bottom: 4px; left: 25%; width: 50%; height:75px; z-index: 33;', // style used for container of audio controls
				startAtFragment: false, // when moving to a slide, start at the current fragment or at the start of the slide
			},
			
			customcontrols: {
					controls: [
      						{
						  id: 'toggle-overview',
						  title: 'Toggle overview (O)',
						  icon: '<i class="fa fa-th"></i>',
						  action: 'Reveal.toggleOverview();'
						}
						
				]
			},
			chart: {
					defaults: { 
						color: 'lightgray', // color of labels
						scale: { 
							beginAtZero: true, 
							ticks: { stepSize: 1 },
							grid: { color: "lightgray" } , // color of grid lines
						},
					},
					line: { borderColor: [ "rgba(20,220,220,.8)" , "rgba(220,120,120,.8)", "rgba(20,120,220,.8)" ], "borderDash": [ [5,10], [0,0] ] }, 
					bar: { backgroundColor: [ "rgba(20,220,220,.8)" , "rgba(220,120,120,.8)", "rgba(20,120,220,.8)" ]}, 
					pie: { backgroundColor: [ ["rgba(0,0,0,.8)" , "rgba(220,20,20,.8)", "rgba(20,220,20,.8)", "rgba(220,220,20,.8)", "rgba(20,20,220,.8)"] ]},
					radar: { borderColor: [ "rgba(20,220,220,.8)" , "rgba(220,120,120,.8)", "rgba(20,120,220,.8)" ]}, 
			},
			math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } }
				},
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: (window.location.search.match(/print-pdf/gi) ? printPlugins : plugins ) 
		});
			


	    // Change chalkboard theme : 
		function changeTheme(input) {
			var config = {};
			config.theme = input.value;
			Reveal.getPlugin("RevealChalkboard").configure(config);
			input.blur();
		}

		// // Handle the message inside the webview
        // window.addEventListener('message', event => {

        //     const message = event.data; // The JSON data our extension sent

        //     switch (message.command) {
        //         case 'refactor':
        //             Reveal.toggleHelp();
        //     }
        // });

		if (window.location.search.match(/print-pdf-now/gi)) {
      		setTimeout(() => {
				window.print();
			  }, 2500);
			
    }
		

	</script>

</body>

</html>